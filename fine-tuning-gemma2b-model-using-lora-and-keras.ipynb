{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9575271,"sourceType":"datasetVersion","datasetId":5837193},{"sourceId":85984,"sourceType":"modelInstanceVersion","modelInstanceId":72244,"modelId":78150}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yahayamkayode/fine-tuning-gemma2b-model-using-lora-and-keras?scriptVersionId=208046475\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<center><h1>Fine-tuning Gemma 2 Model Using LoRA and Keras with Custom Datatset</h1></center>\n\n<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n\n\n# Introduction\n\n> 1. How to fine-tune Gemma model using LoRA with a customise dataset - Maize dataset ğŸŒ½\n> 2. Creation of a specialised class to query about Maize production\n> 3. Some results of querying about best practice for Maize production\n\n#### The following resources were acknowledged for the successful implementation of this project\n\n> 1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n> 2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n> 3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1) \n> 4. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n> 5. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n\n\n\n> **Let's go**ğŸ•ºğŸ•ºğŸ•º\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# What is Gemma 2?\n\n> Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all. \n\n> Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components: \n\n> To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n","metadata":{}},{"cell_type":"markdown","source":"# What is LoRA?  \n\n> **LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times. ","metadata":{}},{"cell_type":"markdown","source":"# How we proceed?\n\n> For fine-tunning with LoRA, we will follow the steps:\n\n> 1. Install prerequisites\n> 2. Load and process the maize data for fine-tuning\n> 3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n> 4. Perform fine-tuning\n> 5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions","metadata":{}},{"cell_type":"markdown","source":"# Prerequisites\n\n\n## Install packages\n\nWe start by installing `keras-nlp` and `keras` packages.","metadata":{}},{"cell_type":"code","source":"# Install dependencies\n!pip install -q -U wurlitzer\n!pip install keras-core\n!pip install -q -U keras-nlp\n!pip install -q -U keras==3.5.0  # Use Keras 3.x to work with JAX\n!pip install -q -U kagglehub --upgrade\n!pip install jax jaxlib\n!pip install keras-nlp\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import packages\n\nNow we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n\nBecause we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\nos.environ[\"JAX_PLATFORMS\"] = \"\"\nimport keras\nimport keras_nlp\nimport kagglehub\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\nos.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas() # progress bar for pandas\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:19:03.366159Z","iopub.execute_input":"2024-11-16T20:19:03.366811Z","iopub.status.idle":"2024-11-16T20:19:03.562567Z","shell.execute_reply.started":"2024-11-16T20:19:03.366771Z","shell.execute_reply":"2024-11-16T20:19:03.561724Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Configurations\n\n\nWe use a `Config` class to group the information needed to control the fine-tuning process:\n* random seed \n* dataset path\n* preset - name of pretrained Gemma 2\n* sequence length - this is the maximum size of input sequence for training\n* batch size - size of the input batch in training, x 2 as two GPUs\n* lora rank - rank for LoRA, higher means more trainable parameters \n* learning rate used in the train\n* epochs - number of epochs for train","metadata":{}},{"cell_type":"code","source":"class Config:\n    seed = 42\n\n    dataset_path = \"/kaggle/input/dataset-maize\"  # Use your dataset's Kaggle path\n    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n    sequence_length = 512 # max size of input sequence for training\n    batch_size = 1 # size of the input batch in training\n    lora_rank = 4 # rank for LoRA, higher means more trainable parameters\n    learning_rate=8e-5 # learning rate used in train\n    epochs = 12 # number of epochs to train\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:19:18.926971Z","iopub.execute_input":"2024-11-16T20:19:18.928133Z","iopub.status.idle":"2024-11-16T20:19:18.93295Z","shell.execute_reply.started":"2024-11-16T20:19:18.92809Z","shell.execute_reply":"2024-11-16T20:19:18.932017Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"keras.utils.set_random_seed(Config.seed)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:19:23.007391Z","iopub.execute_input":"2024-11-16T20:19:23.008273Z","iopub.status.idle":"2024-11-16T20:19:23.012642Z","shell.execute_reply.started":"2024-11-16T20:19:23.008234Z","shell.execute_reply":"2024-11-16T20:19:23.01157Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load the data\n\n\nWe load the data we will use for fine-tunining.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(f\"{Config.dataset_path}/maize-dataset.csv\")\ndf.sample(8)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:19:26.434139Z","iopub.execute_input":"2024-11-16T20:19:26.435139Z","iopub.status.idle":"2024-11-16T20:19:26.470615Z","shell.execute_reply.started":"2024-11-16T20:19:26.435085Z","shell.execute_reply":"2024-11-16T20:19:26.469517Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                              Question  \\\n73   What type of fertilizer should I use for maize...   \n18   What is the recommended planting spacing for m...   \n118  What are the advantages of early planting in m...   \n78   What type of fertilizer should I use for maize...   \n76   What is the ideal watering schedule for maize ...   \n31   What is the ideal watering schedule for maize ...   \n64   How do I manage pests in Southern Guinea Savan...   \n141  What are the signs of nitrogen deficiency in m...   \n\n                                                Answer               Intent  \\\n73   In Lagos, you should use nitrogen-based fertil...       Fertilizer Use   \n18   The recommended planting spacing for maize is ...      Planting Advice   \n118  Early planting helps maize avoid late-season d...    Planting Benefits   \n78   In Osun, you should use nitrogen-based fertili...       Fertilizer Use   \n76   In the Forest zone, maize requires watering ev...    Watering Schedule   \n31   In the Sudan Savanna zone, maize requires wate...    Watering Schedule   \n64   In the Southern Guinea Savanna zone, regular m...         Pest Control   \n141  Nitrogen deficiency in maize is indicated by y...  Deficiency Symptoms   \n\n                                              Entities                Category  \n73                                Lagos, Forest, Maize  Fertilizer Application  \n18                          Maize, Spacing, 75cm, 25cm       Maize Cultivation  \n118  maize, early planting, drought, pest infestations                Agronomy  \n78                                 Osun, Forest, Maize  Fertilizer Application  \n76                                Lagos, Forest, Maize        Water Management  \n31                          Kano, Sudan Savanna, Maize        Water Management  \n64               Benue, Southern Guinea Savanna, Maize            Pest Control  \n141                         maize, nitrogen deficiency     Nutrient Management  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n      <th>Intent</th>\n      <th>Entities</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>73</th>\n      <td>What type of fertilizer should I use for maize...</td>\n      <td>In Lagos, you should use nitrogen-based fertil...</td>\n      <td>Fertilizer Use</td>\n      <td>Lagos, Forest, Maize</td>\n      <td>Fertilizer Application</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>What is the recommended planting spacing for m...</td>\n      <td>The recommended planting spacing for maize is ...</td>\n      <td>Planting Advice</td>\n      <td>Maize, Spacing, 75cm, 25cm</td>\n      <td>Maize Cultivation</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>What are the advantages of early planting in m...</td>\n      <td>Early planting helps maize avoid late-season d...</td>\n      <td>Planting Benefits</td>\n      <td>maize, early planting, drought, pest infestations</td>\n      <td>Agronomy</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>What type of fertilizer should I use for maize...</td>\n      <td>In Osun, you should use nitrogen-based fertili...</td>\n      <td>Fertilizer Use</td>\n      <td>Osun, Forest, Maize</td>\n      <td>Fertilizer Application</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>What is the ideal watering schedule for maize ...</td>\n      <td>In the Forest zone, maize requires watering ev...</td>\n      <td>Watering Schedule</td>\n      <td>Lagos, Forest, Maize</td>\n      <td>Water Management</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>What is the ideal watering schedule for maize ...</td>\n      <td>In the Sudan Savanna zone, maize requires wate...</td>\n      <td>Watering Schedule</td>\n      <td>Kano, Sudan Savanna, Maize</td>\n      <td>Water Management</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>How do I manage pests in Southern Guinea Savan...</td>\n      <td>In the Southern Guinea Savanna zone, regular m...</td>\n      <td>Pest Control</td>\n      <td>Benue, Southern Guinea Savanna, Maize</td>\n      <td>Pest Control</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>What are the signs of nitrogen deficiency in m...</td>\n      <td>Nitrogen deficiency in maize is indicated by y...</td>\n      <td>Deficiency Symptoms</td>\n      <td>maize, nitrogen deficiency</td>\n      <td>Nutrient Management</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"For easiness, we will create the following template for QA: ","metadata":{}},{"cell_type":"code","source":"template = \"\\n\\nQuestion:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n\ndf[\"prompt\"] = df.apply(lambda row: template.format(Question=row.Question,\n                                                    Answer=row.Answer), axis=1)\ndata = df.prompt.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:19:54.106629Z","iopub.execute_input":"2024-11-16T20:19:54.107525Z","iopub.status.idle":"2024-11-16T20:19:54.119618Z","shell.execute_reply.started":"2024-11-16T20:19:54.107469Z","shell.execute_reply":"2024-11-16T20:19:54.118491Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Template utility function","metadata":{}},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Question\", \"Answer\"], [\"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:19:57.625556Z","iopub.execute_input":"2024-11-16T20:19:57.626267Z","iopub.status.idle":"2024-11-16T20:19:57.630909Z","shell.execute_reply.started":"2024-11-16T20:19:57.626224Z","shell.execute_reply":"2024-11-16T20:19:57.62998Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Specialized class to query Gemma\n\n\nWe define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class.","metadata":{}},{"cell_type":"markdown","source":"## Initialize the code for Gemma Causal LM","metadata":{}},{"cell_type":"code","source":"!pip install keras-nlp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_nlp\nprint(keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:21:02.599007Z","iopub.execute_input":"2024-11-16T20:21:02.59943Z","iopub.status.idle":"2024-11-16T20:21:02.605102Z","shell.execute_reply.started":"2024-11-16T20:21:02.599384Z","shell.execute_reply":"2024-11-16T20:21:02.604175Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"0.17.0\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:21:07.135205Z","iopub.execute_input":"2024-11-16T20:21:07.135753Z","iopub.status.idle":"2024-11-16T20:21:45.547015Z","shell.execute_reply.started":"2024-11-16T20:21:07.135712Z","shell.execute_reply":"2024-11-16T20:21:45.546173Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_causal_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:21:58.586609Z","iopub.execute_input":"2024-11-16T20:21:58.58751Z","iopub.status.idle":"2024-11-16T20:21:58.628036Z","shell.execute_reply.started":"2024-11-16T20:21:58.587454Z","shell.execute_reply":"2024-11-16T20:21:58.627242Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              â”‚                      Vocab size: \u001b[38;5;34m256,000\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                                                  </span>â”ƒ<span style=\"font-weight: bold\">                                   Config </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              â”‚                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gemma_backbone                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        â”‚   \u001b[38;5;34m2,614,341,888\u001b[0m â”‚ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        â”‚\nâ”‚ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               â”‚                           â”‚                 â”‚ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_embedding               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      â”‚     \u001b[38;5;34m589,824,000\u001b[0m â”‚ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gemma_backbone                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> â”‚ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_embedding               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> â”‚ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Define the specialized class\n\nHere we define the special class `GemmaQA`. \nin the `__init__` we pass the `GemmaCausalLM` object created before.\nThe `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question.","metadata":{}},{"cell_type":"code","source":"class GemmaQA:\n    def __init__(self, max_length=512):\n        self.max_length = max_length\n        self.prompt = template\n        self.gemma_causal_lm = gemma_causal_lm\n        \n    def query(self, question):\n        response = self.gemma_causal_lm.generate(\n            self.prompt.format(\n                Question=question,\n                Answer=\"\"), \n            max_length=self.max_length)\n        display(Markdown(colorize_text(response)))","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:22:09.226236Z","iopub.execute_input":"2024-11-16T20:22:09.227127Z","iopub.status.idle":"2024-11-16T20:22:09.232867Z","shell.execute_reply.started":"2024-11-16T20:22:09.227082Z","shell.execute_reply":"2024-11-16T20:22:09.231853Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Gemma preprocessor\n\n\nThis preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n\nFrom the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```.","metadata":{}},{"cell_type":"code","source":"x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:22:15.666857Z","iopub.execute_input":"2024-11-16T20:22:15.667267Z","iopub.status.idle":"2024-11-16T20:22:15.768677Z","shell.execute_reply.started":"2024-11-16T20:22:15.667229Z","shell.execute_reply":"2024-11-16T20:22:15.767849Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Perform fine-tuning with LoRA","metadata":{}},{"cell_type":"markdown","source":"## Enable LoRA for the model\n\nLoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\ngemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\ngemma_causal_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:23:35.999933Z","iopub.execute_input":"2024-11-16T20:23:36.000364Z","iopub.status.idle":"2024-11-16T20:23:36.308822Z","shell.execute_reply.started":"2024-11-16T20:23:36.000324Z","shell.execute_reply":"2024-11-16T20:23:36.307887Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              â”‚                      Vocab size: \u001b[38;5;34m256,000\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                                                  </span>â”ƒ<span style=\"font-weight: bold\">                                   Config </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              â”‚                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gemma_backbone                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        â”‚   \u001b[38;5;34m2,617,270,528\u001b[0m â”‚ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        â”‚\nâ”‚ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               â”‚                           â”‚                 â”‚ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_embedding               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      â”‚     \u001b[38;5;34m589,824,000\u001b[0m â”‚ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\nâ”‚ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ gemma_backbone                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> â”‚ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ token_embedding               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> â”‚ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Run the training sequence\n\nWe set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\nWe compile the model, with the loss, optimizer and metric.\nFor the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels.","metadata":{}},{"cell_type":"code","source":"#set sequence length cf. config (512)\ngemma_causal_lm.preprocessor.sequence_length = Config.sequence_length \n\n# Compile the model with loss, optimizer, and metric\ngemma_causal_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train model\ngemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:23:41.570142Z","iopub.execute_input":"2024-11-16T20:23:41.570821Z","iopub.status.idle":"2024-11-16T20:49:31.760388Z","shell.execute_reply.started":"2024-11-16T20:23:41.57078Z","shell.execute_reply":"2024-11-16T20:49:31.759428Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 947ms/step - loss: 0.1787 - sparse_categorical_accuracy: 0.5858\nEpoch 2/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 820ms/step - loss: 0.0803 - sparse_categorical_accuracy: 0.7630\nEpoch 3/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0621 - sparse_categorical_accuracy: 0.8112\nEpoch 4/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0557 - sparse_categorical_accuracy: 0.8267\nEpoch 5/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0496 - sparse_categorical_accuracy: 0.8390\nEpoch 6/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0442 - sparse_categorical_accuracy: 0.8557\nEpoch 7/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.8669\nEpoch 8/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0340 - sparse_categorical_accuracy: 0.8849\nEpoch 9/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0287 - sparse_categorical_accuracy: 0.9011\nEpoch 10/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9148\nEpoch 11/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9262\nEpoch 12/12\n\u001b[1m150/150\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 820ms/step - loss: 0.0188 - sparse_categorical_accuracy: 0.9299\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x79d1b40df6d0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Test the fine-tuned model\n\nWe instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model.","metadata":{}},{"cell_type":"code","source":"gemma_qa = GemmaQA()","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:54:40.06368Z","iopub.execute_input":"2024-11-16T20:54:40.064232Z","iopub.status.idle":"2024-11-16T20:54:40.070126Z","shell.execute_reply.started":"2024-11-16T20:54:40.06416Z","shell.execute_reply":"2024-11-16T20:54:40.069048Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"For start, we will testing the model with some of the data from the training set itself.","metadata":{}},{"cell_type":"code","source":"row = df.iloc[0]\ngemma_qa.query(row.Question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:54:45.54325Z","iopub.execute_input":"2024-11-16T20:54:45.543641Z","iopub.status.idle":"2024-11-16T20:55:02.080938Z","shell.execute_reply.started":"2024-11-16T20:54:45.543604Z","shell.execute_reply":"2024-11-16T20:55:02.07986Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the best soil for maize?\n\n**<font color='green'>Answer:</font>**\nMaize grows best in well-drained loamy or sandy soil with high organic matter content."},"metadata":{}}]},{"cell_type":"code","source":"row = df.iloc[3]\ngemma_qa.query(row.Question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:55:32.236497Z","iopub.execute_input":"2024-11-16T20:55:32.2374Z","iopub.status.idle":"2024-11-16T20:55:33.98471Z","shell.execute_reply.started":"2024-11-16T20:55:32.237357Z","shell.execute_reply":"2024-11-16T20:55:33.983684Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nHow can I control weeds in my maize farm?\n\n**<font color='green'>Answer:</font>**\nTo control weeds in maize, use herbicides like Atrazine or Halosulfuron at 2-leaf or 2-3 leaf stage. Regular weeding is also important."},"metadata":{}}]},{"cell_type":"code","source":"row = df.iloc[105]\ngemma_qa.query(row.Question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:55:39.415863Z","iopub.execute_input":"2024-11-16T20:55:39.416551Z","iopub.status.idle":"2024-11-16T20:55:40.792627Z","shell.execute_reply.started":"2024-11-16T20:55:39.41651Z","shell.execute_reply":"2024-11-16T20:55:40.791622Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the ideal pH for maize cultivation?\n\n**<font color='green'>Answer:</font>**\nThe ideal pH for maize cultivation is between 5.5 and 7.0. Adjust soil acidity with lime if necessary."},"metadata":{}}]},{"cell_type":"markdown","source":"## Test the model with unseen question(s)","metadata":{}},{"cell_type":"code","source":"question = \"What is the best time to plant Maize?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:55:43.045908Z","iopub.execute_input":"2024-11-16T20:55:43.047089Z","iopub.status.idle":"2024-11-16T20:55:44.979135Z","shell.execute_reply.started":"2024-11-16T20:55:43.047031Z","shell.execute_reply":"2024-11-16T20:55:44.978132Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the best time to plant Maize?\n\n**<font color='green'>Answer:</font>**\nThe best time to plant Maize is in the early rainy season, about 2-3 weeks before the first rainfall. This allows the crop to take advantage of the moisture provided by the early rains."},"metadata":{}}]},{"cell_type":"code","source":"question = \"How many seed of maize should I plant per hole?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:55:51.043624Z","iopub.execute_input":"2024-11-16T20:55:51.044539Z","iopub.status.idle":"2024-11-16T20:55:52.403859Z","shell.execute_reply.started":"2024-11-16T20:55:51.044497Z","shell.execute_reply":"2024-11-16T20:55:52.402669Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nHow many seed of maize should I plant per hole?\n\n**<font color='green'>Answer:</font>**\nPlant about 2-3 seeds per hole, spacing them 50-60 cm apart for good plant spacing."},"metadata":{}}]},{"cell_type":"code","source":"question = \"What is the ideal spacing for maize planting?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:55:54.691253Z","iopub.execute_input":"2024-11-16T20:55:54.69166Z","iopub.status.idle":"2024-11-16T20:55:55.87108Z","shell.execute_reply.started":"2024-11-16T20:55:54.691622Z","shell.execute_reply":"2024-11-16T20:55:55.870088Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the ideal spacing for maize planting?\n\n**<font color='green'>Answer:</font>**\nFor optimal spacing, plant maize at 75 cm between rows and 25 cm between plants."},"metadata":{}}]},{"cell_type":"code","source":"question = \"What is the right time to harvest maize?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:55:57.415418Z","iopub.execute_input":"2024-11-16T20:55:57.415824Z","iopub.status.idle":"2024-11-16T20:55:58.794153Z","shell.execute_reply.started":"2024-11-16T20:55:57.415786Z","shell.execute_reply":"2024-11-16T20:55:58.793167Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the right time to harvest maize?\n\n**<font color='green'>Answer:</font>**\nMaize can be harvested when the silks turn brown, the husks turn yellow, or when the grains are hard and dry."},"metadata":{}}]},{"cell_type":"code","source":"question = \"What is the best way to store maize after harvest?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:56:03.403473Z","iopub.execute_input":"2024-11-16T20:56:03.404266Z","iopub.status.idle":"2024-11-16T20:56:06.48655Z","shell.execute_reply.started":"2024-11-16T20:56:03.404226Z","shell.execute_reply":"2024-11-16T20:56:06.485566Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the best way to store maize after harvest?\n\n**<font color='green'>Answer:</font>**\nAfter harvesting maize, it is important to dry the grains quickly to prevent mold. Spread the dried maize in a well-ventilated area or use a maize dryer to ensure proper storage. Store the maize in airtight containers or in a cool, dry place, such as a ventilated granary or a dry room, to protect it from pests and rodents."},"metadata":{}}]},{"cell_type":"code","source":"question = \"How do I manage Fall Army Worm on maize crop?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:58:46.949633Z","iopub.execute_input":"2024-11-16T20:58:46.950628Z","iopub.status.idle":"2024-11-16T20:58:50.054391Z","shell.execute_reply.started":"2024-11-16T20:58:46.950582Z","shell.execute_reply":"2024-11-16T20:58:50.053191Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nHow do I manage Fall Army Worm on maize crop?\n\n**<font color='green'>Answer:</font>**\nFall Army Worm can be managed through early detection, using pheromone traps, and applying insecticides like Spinosad or\n\n<h2>Related Posts</h2>\n\n* How to Manage Fall Army Worm in Maize\n* How to Use Pheromone Traps for Fall Army Worm in Maize\n* What are the Recommended Insecticides for Fall Army Worm in Maize?"},"metadata":{}}]},{"cell_type":"code","source":"question = \"How do I get rid-off yellow maize leave?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:59:00.550626Z","iopub.execute_input":"2024-11-16T20:59:00.551013Z","iopub.status.idle":"2024-11-16T20:59:02.301575Z","shell.execute_reply.started":"2024-11-16T20:59:00.550975Z","shell.execute_reply":"2024-11-16T20:59:02.300574Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nHow do I get rid-off yellow maize leave?\n\n**<font color='green'>Answer:</font>**\nYellow maize leaves can be a sign of nutrient deficiency, particularly iron or magnesium. To address this, apply a foliar spray of iron sulfate or use a magnesium-based fertilizer."},"metadata":{}}]},{"cell_type":"code","source":"question = \"How can I identify bad maize seed not to plant?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:59:08.495739Z","iopub.execute_input":"2024-11-16T20:59:08.496498Z","iopub.status.idle":"2024-11-16T20:59:09.832602Z","shell.execute_reply.started":"2024-11-16T20:59:08.496453Z","shell.execute_reply":"2024-11-16T20:59:09.831651Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nHow can I identify bad maize seed not to plant?\n\n**<font color='green'>Answer:</font>**\nBad maize seeds can be identified by wilting leaves, reduced vigor, or by physical defects such as mold or insect damage."},"metadata":{}}]},{"cell_type":"code","source":"question = \"What is the best maize variety in Abuja for planting\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:59:14.415259Z","iopub.execute_input":"2024-11-16T20:59:14.415935Z","iopub.status.idle":"2024-11-16T20:59:15.912225Z","shell.execute_reply.started":"2024-11-16T20:59:14.415893Z","shell.execute_reply":"2024-11-16T20:59:15.911289Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the best maize variety in Abuja for planting\n\n**<font color='green'>Answer:</font>**\nIn Abuja, SAMMAZ 14, SAMMAZ 41, and SAMMAZ 10 are the recommended maize varieties for planting."},"metadata":{}}]},{"cell_type":"code","source":"question = \"What is the best maize variety in Kaduna state?\"\ngemma_qa.query(question)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T20:59:18.303289Z","iopub.execute_input":"2024-11-16T20:59:18.303686Z","iopub.status.idle":"2024-11-16T20:59:19.721461Z","shell.execute_reply.started":"2024-11-16T20:59:18.303648Z","shell.execute_reply":"2024-11-16T20:59:19.720389Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>**\nWhat is the best maize variety in Kaduna state?\n\n**<font color='green'>Answer:</font>**\nIn Kaduna, SAMMAZ 14, SAMMAZ 21, and SAMMAZ 23 are the recommended maize varieties."},"metadata":{}}]},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"preset_dir = \".\\gemma2_2b_en_maize_model\"\ngemma_causal_lm.save_to_preset(preset_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T21:01:43.658264Z","iopub.execute_input":"2024-11-16T21:01:43.659173Z","iopub.status.idle":"2024-11-16T21:02:29.353512Z","shell.execute_reply.started":"2024-11-16T21:01:43.659128Z","shell.execute_reply":"2024-11-16T21:02:29.352389Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Publish the model on Kaggle as a Kaggle Model\n\nWe are publishing now the saved model as a Kaggle Model.","metadata":{}},{"cell_type":"code","source":"kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\nprint(kaggle_username)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T21:04:27.747914Z","iopub.execute_input":"2024-11-16T21:04:27.748686Z","iopub.status.idle":"2024-11-16T21:04:27.753568Z","shell.execute_reply.started":"2024-11-16T21:04:27.748642Z","shell.execute_reply":"2024-11-16T21:04:27.75263Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"yahayamkayode\n","output_type":"stream"}]},{"cell_type":"code","source":"kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n\nkaggle_uri = f\"kaggle://{kaggle_username}/maize-dataset/keras/gemma2_2b_en_maize_model\"\n\n# Proceed with the upload\nkeras_nlp.upload_preset(kaggle_uri, preset_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T21:04:29.5554Z","iopub.execute_input":"2024-11-16T21:04:29.555795Z","iopub.status.idle":"2024-11-16T21:06:33.771492Z","shell.execute_reply.started":"2024-11-16T21:04:29.555757Z","shell.execute_reply":"2024-11-16T21:06:33.770503Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Uploading Model https://www.kaggle.com/models/yahayamkayode/maize-dataset/keras/gemma2_2b_en_maize_model ...\nModel 'maize-dataset' does not exist or access is forbidden for user 'yahayamkayode'. Creating or handling Model...\nModel 'maize-dataset' Created.\nStarting upload for file .\\gemma2_2b_en_maize_model/task.json\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.98k/2.98k [00:00<00:00, 14.5kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/task.json (3KB)\nStarting upload for file .\\gemma2_2b_en_maize_model/preprocessor.json\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.41k/1.41k [00:00<00:00, 8.40kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/preprocessor.json (1KB)\nStarting upload for file .\\gemma2_2b_en_maize_model/model.weights.h5\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.5G/10.5G [01:59<00:00, 87.5MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/model.weights.h5 (10GB)\nStarting upload for file .\\gemma2_2b_en_maize_model/config.json\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:00<00:00, 2.66kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/config.json (782B)\nStarting upload for file .\\gemma2_2b_en_maize_model/metadata.json\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 812B/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/metadata.json (143B)\nStarting upload for file .\\gemma2_2b_en_maize_model/tokenizer.json\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591/591 [00:00<00:00, 3.39kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/tokenizer.json (591B)\nStarting upload for file .\\gemma2_2b_en_maize_model/assets/tokenizer/vocabulary.spm\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.24M/4.24M [00:00<00:00, 12.7MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: .\\gemma2_2b_en_maize_model/assets/tokenizer/vocabulary.spm (4MB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Your model instance has been created.\nFiles are being processed...\nSee at: https://www.kaggle.com/models/yahayamkayode/maize-dataset/keras/gemma2_2b_en_maize_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Upload the model to Hugging Face","metadata":{}},{"cell_type":"code","source":"# Upload the preset to Hugging Face Hub\nhf_uri = \"hf://Justsp3cial/maize_model\"\nkeras_nlp.upload_preset(hf_uri, '.\\\\gemma2_2b_en_maize_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\n","metadata":{}},{"cell_type":"markdown","source":"> - Fine-tuning of a **Gemma 2** model has been demonstated using LoRA.   \n> -  A class was alos created to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, unseen questions.   \n> - The models was as a Keras model.\n> - The model was evaluated using Perplexity,recorded Perplexity value of 2.601. \n> - Finnally, the model was published as a Kaggle Model on Kaggle Models platform.","metadata":{}}]}